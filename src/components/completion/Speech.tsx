import { getSettings, transcribeAudio } from "@/lib";
import { CompletionState } from "@/types";
import { useMicVAD } from "@ricky0123/vad-react";
import { LoaderCircleIcon, MicIcon, MicOffIcon } from "lucide-react";
import React, { useState } from "react";
import { Button } from "../ui/button";

export const Speech = ({
  submit,
  setState,
  setEnableVAD,
}: {
  submit: (transcription: string) => void;
  setState: React.Dispatch<React.SetStateAction<CompletionState>>;
  setEnableVAD: React.Dispatch<React.SetStateAction<boolean>>;
}) => {
  const [isTranscribing, setIsTranscribing] = useState(false);

  const vad = useMicVAD({
    userSpeakingThreshold: 0.6,
    startOnLoad: true,
    onSpeechEnd: async (audio) => {
      console.log("ğŸ” VAD onSpeechEnd TRIGGERED!");
      console.log("ğŸ” Audio data:", audio);
      console.log("ğŸ” Audio type:", typeof audio);
      console.log("ğŸ” Audio length:", audio?.length);
      
      try {
        console.log("ğŸ” Starting transcription process...");
        setIsTranscribing(true);
        const settings = getSettings();
        console.log("ğŸ” Settings retrieved:", settings);

        // Check if we have an OpenAI API key for transcription
        let openAiKey = "";
        if (settings.selectedProvider === "openai") {
          console.log("ğŸ” Using OpenAI provider");
          if (!settings?.apiKey || !settings?.isApiKeySubmitted) {
            console.warn("ğŸ” No OpenAI API key configured for transcription");
            return;
          }
          openAiKey = settings.apiKey;
          console.log("ğŸ” OpenAI API key found, length:", openAiKey.length);
        } else {
          console.log("ğŸ” Using separate OpenAI key for provider:", settings.selectedProvider);
          if (!settings?.openAiApiKey || !settings?.isOpenAiApiKeySubmitted) {
            console.warn("ğŸ” No OpenAI API key configured for speech-to-text");
            return;
          }
          openAiKey = settings.openAiApiKey;
          console.log("ğŸ” Separate OpenAI API key found, length:", openAiKey.length);
        }

        console.log("ğŸ” Calling transcribeAudio...");
        const transcription = await transcribeAudio(audio, openAiKey);
        console.log("ğŸ” Transcription result:", transcription);
        
        if (transcription) {
          console.log("ğŸ” Submitting transcription:", transcription);
          submit(transcription);
        } else {
          console.warn("ğŸ” No transcription returned");
        }
      } catch (error) {
        console.error("ğŸ” ERROR in transcription:", error);
        setState((prev) => ({
          ...prev,
          error:
            error instanceof Error ? error.message : "Transcription failed",
        }));
      } finally {
        setIsTranscribing(false);
        console.log("ğŸ” Transcription process completed");
      }
    },
  });

  // Add VAD state debugging
  React.useEffect(() => {
    console.log("ğŸ” VAD State Debug:");
    console.log("ğŸ” VAD listening:", vad.listening);
    console.log("ğŸ” VAD userSpeaking:", vad.userSpeaking);
    console.log("ğŸ” VAD loading:", vad.loading);
    console.log("ğŸ” VAD error:", vad.error);
  }, [vad.listening, vad.userSpeaking, vad.loading, vad.error]);

  return (
    <>
      <Button
        size="icon"
        onClick={() => {
          if (vad.listening) {
            vad.pause();
            setEnableVAD(false);
          } else {
            vad.start();
            setEnableVAD(true);
          }
        }}
        className="cursor-pointer"
      >
        {isTranscribing ? (
          <LoaderCircleIcon className="h-4 w-4 animate-spin text-green-500" />
        ) : vad.userSpeaking ? (
          <LoaderCircleIcon className="h-4 w-4 animate-spin" />
        ) : vad.listening ? (
          <MicOffIcon className="h-4 w-4 animate-pulse" />
        ) : (
          <MicIcon className="h-4 w-4" />
        )}
      </Button>
    </>
  );
};
